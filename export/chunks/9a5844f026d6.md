---
chunk_index: 423
ref: "9a5844f026d6"
id: "9a5844f026d6789f1d81ad7cd09d032f2e3d9c8136ed9361ac12dab2daee177f"
slug: "phase6-status--1-burn-model-inference"
path: "/home/zack/dev/llmx/docs/PHASE6_STATUS.md"
kind: "markdown"
lines: [111, 147]
token_estimate: 283
content_sha256: "1bae8b4c1358475a7a1ec54c8a636c1930d96824b441aa64598e477149581096"
compacted: false
heading_path: ["Phase 6 Implementation Status","ðŸš§ Blocked Components","1. Burn Model Inference"]
symbol: null
address: null
asset_path: null
---

### 1. Burn Model Inference

**Issue:** `burn-import` requires ONNX opset 13+, but `bge-small-en-v1.5` ONNX model uses opset 11.

**Error:**
```
Failed to parse ONNX file: UnsupportedOpset { required: 13, actual: 11 }
```

**Why This Blocks:**
- Cannot generate Burn model code from ONNX
- Cannot complete forward pass implementation
- Cannot test actual embeddings in browser

**Potential Solutions:**

1. **Convert ONNX to opset 13** (Recommended)
   ```bash
   # Using onnx-simplifier and onnxruntime tools
   pip install onnx onnx-simplifier
   python -m onnxsim model.onnx model_opset13.onnx --opset 13
   ```

2. **Use alternative model**
   - Find a bge-small-en-v1.5 export with opset 13+
   - Use nomic-embed-text-v1.5 if available with correct opset
   - Export model ourselves from PyTorch

3. **Use older Burn version**
   - Burn 0.13/0.14 might support opset 11
   - Trade-off: loses newer features, dependency conflicts

4. **Alternative inference engine**
   - Use `tract` crate instead of Burn (supports more opsets)
   - Use `ort-wasm` (ONNX Runtime for WebAssembly)
   - Trade-off: Different architecture than Burn