---
chunk_index: 1206
ref: "418a1eab9b41"
id: "418a1eab9b4122cc573b978d82375ee10525fbf354f43387abc6144b99d463df"
slug: "smoke-tests-l250-313"
path: "/home/zack/dev/llmx/ingestor-core/tests/smoke_tests.rs"
kind: "text"
lines: [250, 313]
token_estimate: 520
content_sha256: "8b3fa8109a886d626426d1cafcf5c0096858f1b67a6d69b14a8cf088cd44ca01"
compacted: false
heading_path: []
symbol: null
address: null
asset_path: null
---

for (name, content) in files {
        fs::write(project.path().join(name), content).unwrap();
    }

    let idx_input = IndexInput {
        paths: vec![project.path().to_string_lossy().to_string()],
        options: None,
    };
    let idx_output = llmx_index_handler(&mut store, idx_input).expect("Index should succeed");

    assert_eq!(idx_output.stats.total_files, 7, "Should index all 7 files");
    assert!(idx_output.warnings.is_empty(), "Should have no warnings");
}

#[test]
fn smoke_token_budget_respected() {
    let storage_temp = TempDir::new().unwrap();
    let mut store = IndexStore::new(storage_temp.path().to_path_buf()).unwrap();

    // Create a project with multiple large files
    let project = TempDir::new().unwrap();
    for i in 0..10 {
        let content = format!(
            "// File {}\n{}",
            i,
            "fn function() { /* lots of code */ }\n".repeat(100)
        );
        fs::write(project.path().join(format!("file{}.rs", i)), content).unwrap();
    }

    let idx_input = IndexInput {
        paths: vec![project.path().to_string_lossy().to_string()],
        options: None,
    };
    let idx_output = llmx_index_handler(&mut store, idx_input).unwrap();

    // Search with small token budget
    let search_input = SearchInput {
        index_id: idx_output.index_id,
        query: "function".to_string(),
        filters: None,
        limit: Some(100),
        max_tokens: Some(500), // Very small
        use_semantic: None,
    };
    let search_output = llmx_search_handler(&mut store, search_input).unwrap();

    // Should have results and possibly truncated IDs
    assert!(search_output.total_matches > 0);

    // Calculate total tokens in results
    let total_content_len: usize = search_output
        .results
        .iter()
        .map(|r| r.content.len())
        .sum();

    // Should be roughly within budget (with some overhead for formatting)
    // 500 tokens * ~4 chars/token = ~2000 chars
    assert!(
        total_content_len < 3000,
        "Content should be within token budget"
    );
}