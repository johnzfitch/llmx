---
chunk_index: 44
ref: "2e7160f272ff"
id: "2e7160f272ff116eb92b3d8f7f7b30ce3415706444f3dd2e9a4f8c5b96207ae3"
slug: "claude-model-selection-for-c--assistant"
path: "/home/zack/dev/llmx/claude_Model selection for current project_2026-01-17.md"
kind: "markdown"
lines: [125, 152]
token_estimate: 558
content_sha256: "ffa12bcbd2a5d4e681017cd9ea52548f95326b349ac4e6313800c76ea4d1f2ba"
compacted: false
heading_path: ["Model selection for current project","Assistant"]
symbol: null
address: null
asset_path: null
---

## Assistant

The user is asking if the embedding model recommendations will work on llm.cat. Let me search for context about llm.cat to understand what constraints it has.
The search didn't give me clear info about llm.cat's architecture constraints. Let me search more specifically for what llm.cat is and what its deployment environment looks like.
Ah, I see. Based on these search results, llm.cat appears to be a client-side browser-based tool, potentially using WebAssembly/WebGPU. If it's meant to run in the browser, then ONNX models need to work with ONNX Runtime Web (ort-web), not the native ort crate.

Let me search for more specific info about llm.cat's architecture.
Based on the search results, llm.cat appears to be a web-based tool for generating llms.txt files. From the earlier search showing WebGPU/WebAssembly discussions and the architecture diagram showing "User uploads folder → Process files → Generate llms.txt + index.json", it seems like llm.cat could be either:

1. A client-side browser-based tool (like the WebGPU image detector project)
2. A server-side tool

The search results also mention integration plans for llm.cat with specho-v2's Layer E quality scoring, which is dependency-free pure Rust.

I should ask the user to clarify what llm.cat's architecture is - is it running in the browser (WASM), or on a server? The embedding model recommendations I gave are for native Rust with the `ort` crate, which won't work in WASM. If llm.cat is browser-based, we'd need to use `onnxruntime-web` (JavaScript) or a different approach entirely.Good question. What's llm.cat's runtime environment?

If it's **browser-based (WASM)**, the P6 directions won't work as-is. The `ort` crate doesn't compile to WASM. You'd need:

- **onnxruntime-web** (JS) - works but you'd be mixing Rust WASM + JS inference
- **Burn with WGPU backend** - Rust-native, compiles to WASM, uses WebGPU
- **candle** (Hugging Face Rust) - has WASM support, lighter than ort

For browser, the model size also matters more. 33MB (bge-small) is fine. 137MB (nomic) is pushing it for first-load UX.

If it's **server-side** (e.g., running on your FrankenPHP container), the directions work as written.

What's the deployment target?