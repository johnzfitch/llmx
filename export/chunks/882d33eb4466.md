---
chunk_index: 40
ref: "882d33eb4466"
id: "882d33eb44663f3ec9f71ef74358850fdac6d9f8e679ac3a04b939583e32cebf"
slug: "claude-model-selection-for-c--assistant"
path: "/home/zack/dev/llmx/claude_Model selection for current project_2026-01-17.md"
kind: "markdown"
lines: [9, 63]
token_estimate: 891
content_sha256: "984546ab8f8393079a94f430e31b728975d24597c7f9b151604916e50c79214e"
compacted: false
heading_path: ["Model selection for current project","Assistant"]
symbol: null
address: null
asset_path: null
---

## Assistant

The user is asking about whether sentence-transformers/all-MiniLM-L6-v2 is the right model for a project, and they're referencing a recent chat about "Phase 4 directions." Let me search for that context first.
Got it. This is the llmx project (a codebase indexing MCP server in Rust). Phase 5 is about adding semantic search/embeddings. The directions mention `all-MiniLM-L6` as one of the local ONNX model options, which is what the user is asking about.

The user is asking if `sentence-transformers/all-MiniLM-L6-v2` is the right model for this. They note it's a bit dated - which is correct. This model was released in 2021 and has 384 dimensions.

Let me give them an informed opinion on this:

**all-MiniLM-L6-v2**:
- 384 dimensions
- ~22M parameters
- Released 2021
- Fast, small, but lower quality than newer models

**Better options for code search in 2024-2025**:
1. **Nomic Embed** (nomic-embed-text-v1.5) - 768 dims, very good, Apache 2.0
2. **Voyage AI code-3** - specifically optimized for code, excellent quality but API-only
3. **e5-small-v2** (intfloat/e5-small-v2) - good balance, 384 dims, better than MiniLM
4. **e5-base-v2** or **e5-large-v2** - better quality, 768/1024 dims
5. **jina-embeddings-v3** - new, multilingual, good quality
6. **gte-small** or **gte-base** (thenlper) - very competitive
7. **BGE models** (BAAI/bge-small-en-v1.5) - also very good

For code specifically:
- **Voyage code-3** is probably the best quality but requires API
- **CodeBERT** variants exist but haven't aged as well


- **StarCoder2** has embeddings available but they're quite large

For local ONNX deployment, I'd prioritize **e5-small-v2** as a direct upgrade from MiniLM, or **gte-small** and **bge-small-en-v1.5** which are both excellent alternatives. **Nomic Embed** is also solid and fully local-friendly. With a 4090 and Arch Linux, there's plenty of headroom to run larger models if ONNX runtime supports them efficiently.

For Rust integration, the `ort` crate provides the main ONNX runtime bindings, while Hugging Face's `candle` offers another path for local inference. The embedding-specific approach differs from typical LLM inference patterns like llama.cpp. I should highlight why these modern options outperform older baselines and which fits best for the use case.Found it - Phase 5 directions for llmx list `all-MiniLM-L6` as a local ONNX option.

You're right, it's dated (2021, 384-dim). For code search specifically, there are better options now:

**Direct local upgrades** (ONNX-friendly, similar footprint):
- **`bge-small-en-v1.5`** (BAAI) - 384-dim, outperforms MiniLM significantly on MTEB benchmarks
- **`gte-small`** (thenlper) - 384-dim, Alibaba DAMO, excellent quality/speed ratio
- **`e5-small-v2`** (intfloat) - 384-dim, Microsoft, better contrastive training

**If you can afford more dimensions** (4090 makes this trivial):
- **`nomic-embed-text-v1.5`** - 768-dim, Apache 2.0, supports Matryoshka (can truncate to 256/512 at inference)
- **`bge-base-en-v1.5`** - 768-dim, sweet spot for quality
- **`jina-embeddings-v3`** - 1024-dim, task-specific LoRA adapters, very recent

**Code-specific**:
- **Voyage `code-3`** - best quality for code, but API-only ($$$)
- **CodeSage** (codesage-small-v2) - specifically trained on code, 1024-dim

For llmx I'd probably go **`bge-small-en-v1.5`** as the default (drop-in replacement, same 384-dim) and **`nomic-embed-text-v1.5`** as the quality option. Both work great with `ort` crate.

Want me to update the Phase 5 directions with better model recommendations?