---
chunk_index: 247
ref: "a13a45b4a2f5"
id: "a13a45b4a2f57383732651da3ecc0ddfd583ce525572fc1d107a318ad56f9c16"
slug: "llmcat-integration-plan--3-1-quality-weighted-search-ranking"
path: "/home/zack/dev/llmx/docs/LLMCAT_INTEGRATION_PLAN.md"
kind: "markdown"
lines: [349, 377]
token_estimate: 210
content_sha256: "6b3d2ae0f1421a2d13fac9bb7fd5397e7cf8f9e30f488115ccd4bc72a9567354"
compacted: false
heading_path: ["specho-v2 Integration for llm.cat: Token Efficiency Implementation","Phase 3: Enhanced Search Integration","3.1: Quality-Weighted Search Ranking"]
symbol: null
address: null
asset_path: null
---

### 3.1: Quality-Weighted Search Ranking

**When LLMs search the index**:

```python
def search_with_quality(query: str, chunks: List[Dict]) -> List[Dict]:
    """
    Combine BM25 relevance with quality scores.
    
    Final score = (BM25_score * 0.6) + (quality_score * 0.4)
    """
    # Traditional BM25 search
    bm25_results = bm25_search(query, chunks)  # returns [(chunk, score)]
    
    # Re-rank with quality
    for chunk, bm25_score in bm25_results:
        quality_score = chunk.get('quality_score', 0.5)
        chunk['final_score'] = (bm25_score * 0.6) + (quality_score * 0.4)
    
    # Sort by final score
    bm25_results.sort(key=lambda x: x[0]['final_score'], reverse=True)
    
    return [chunk for chunk, _ in bm25_results]
```

**Result**: LLMs find the best chunk faster, not just the most relevant chunk.

---