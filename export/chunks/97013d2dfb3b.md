---
chunk_index: 1209
ref: "97013d2dfb3b"
id: "97013d2dfb3b69c6f03aaf63b7a23fae5b6cdc0887ac22d7aa56daab9f25f700"
slug: "token-savings-tests-l281-413"
path: "/home/zack/dev/llmx/ingestor-core/tests/token_savings_tests.rs"
kind: "text"
lines: [281, 413]
token_estimate: 1005
content_sha256: "32aae9063b4115c4e59bfaebf8d34aa920c9396262e98aa9b4ff880bdd949975"
compacted: false
heading_path: []
symbol: null
address: null
asset_path: null
---

println!("Search vs Grep comparison:");
    println!("  Grep tokens: {}", grep_tokens);
    println!("  Search tokens: {}", search_tokens);

    // llmx should be more efficient than raw grep
    assert!(
        search_tokens < grep_tokens,
        "llmx search should use fewer tokens than grep"
    );
}

// ============================================================================
// Get Chunk vs Read File Comparison
// ============================================================================

#[test]
fn test_token_savings_get_vs_read() {
    let (_storage, mut store) = create_store();
    let project = TempDir::new().unwrap();

    // Create a larger file
    let content = format!(
        "// Large file\n\n{}\n",
        r#"
/// Documentation for function.
pub fn important_function() {
    let step1 = prepare_data();
    let step2 = process(step1);
    let step3 = finalize(step2);
    step3
}
"#
        .repeat(100)
    );
    fs::write(project.path().join("large.rs"), &content).unwrap();

    let raw_tokens = estimate_tokens(&content);

    // Index
    let idx_input = IndexInput {
        paths: vec![project.path().to_string_lossy().to_string()],
        options: None,
    };
    let idx_output = llmx_index_handler(&mut store, idx_input).unwrap();

    // Get specific chunk via search
    let search_input = SearchInput {
        index_id: idx_output.index_id.clone(),
        query: "important_function".to_string(),
        filters: None,
        limit: Some(1),
        max_tokens: Some(16000),
        use_semantic: None,
    };
    let search_output = llmx_search_handler(&mut store, search_input).unwrap();

    let chunk_tokens: usize = search_output
        .results
        .iter()
        .map(|r| estimate_tokens(&r.content))
        .sum();

    let savings_pct = if raw_tokens > 0 {
        ((raw_tokens - chunk_tokens) as f64 / raw_tokens as f64) * 100.0
    } else {
        0.0
    };

    println!("Get chunk vs read file:");
    println!("  Full file tokens: {}", raw_tokens);
    println!("  Chunk tokens: {}", chunk_tokens);
    println!("  Savings: {:.1}%", savings_pct);

    // Getting a specific chunk should be much smaller than full file
    assert!(
        savings_pct >= 50.0,
        "Chunk retrieval should save at least 50%, got {:.1}%",
        savings_pct
    );
}

// ============================================================================
// Explore Mode Token Efficiency
// ============================================================================

#[test]
fn test_token_savings_explore_vs_tree() {
    let (_storage, mut store) = create_store();
    let project = TempDir::new().unwrap();

    // Create nested directory structure
    let dirs = ["src", "src/api", "src/db", "src/utils", "tests", "docs"];
    for dir in dirs {
        fs::create_dir_all(project.path().join(dir)).unwrap();
        fs::write(
            project.path().join(dir).join("mod.rs"),
            "// Module\npub fn init() {}",
        )
        .unwrap();
    }

    // Simulate tree output (full paths)
    let tree_output = dirs
        .iter()
        .map(|d| format!("{}/mod.rs\n", d))
        .collect::<String>();
    let tree_tokens = estimate_tokens(&tree_output);

    // llmx explore files
    let idx_input = IndexInput {
        paths: vec![project.path().to_string_lossy().to_string()],
        options: None,
    };
    let idx_output = llmx_index_handler(&mut store, idx_input).unwrap();

    let explore_input = ingestor_core::handlers::ExploreInput {
        index_id: idx_output.index_id,
        mode: "files".to_string(),
        path_filter: None,
    };
    let explore_output =
        ingestor_core::handlers::llmx_explore_handler(&mut store, explore_input).unwrap();

    let explore_tokens: usize = explore_output
        .items
        .iter()
        .map(|p| estimate_tokens(p))
        .sum();

    println!("Explore vs tree:");
    println!("  Tree tokens: {}", tree_tokens);
    println!("  Explore tokens: {}", explore_tokens);