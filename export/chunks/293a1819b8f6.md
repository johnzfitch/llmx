---
chunk_index: 249
ref: "293a1819b8f6"
id: "293a1819b8f668bea867e496bb715a7ed826e70af11e6d622e67b46bdc83b72f"
slug: "llmcat-integration-plan--4-1-smart-truncation"
path: "/home/zack/dev/llmx/docs/LLMCAT_INTEGRATION_PLAN.md"
kind: "markdown"
lines: [380, 418]
token_estimate: 250
content_sha256: "31435bfeeb784d0ea47bf22d67206680e1fd47e94a2a5add1b3018353219f294"
compacted: false
heading_path: ["specho-v2 Integration for llm.cat: Token Efficiency Implementation","Phase 4: Token Budget Optimization","4.1: Smart Truncation"]
symbol: null
address: null
asset_path: null
---

### 4.1: Smart Truncation

**File**: `backend/token_optimizer.py`

```python
def optimize_for_token_budget(chunks: List[Dict], max_tokens: int) -> List[Dict]:
    """
    Select best chunks that fit within token budget.
    
    Args:
        chunks: Chunks with quality_score and BM25 ranking
        max_tokens: Maximum total tokens allowed
    
    Returns:
        Optimized subset of chunks
    """
    # Sort by combined score
    chunks.sort(key=lambda c: c.get('final_score', c.get('quality_score', 0)), 
                reverse=True)
    
    selected = []
    total_tokens = 0
    
    for chunk in chunks:
        chunk_tokens = len(chunk['content']) // 4  # rough estimate
        if total_tokens + chunk_tokens <= max_tokens:
            selected.append(chunk)
            total_tokens += chunk_tokens
        else:
            break
    
    return selected

# Usage: When LLM has 8K token context limit
top_chunks = optimize_for_token_budget(search_results, max_tokens=8000)
```

---