---
chunk_index: 74
ref: "c3184f7455d5"
id: "c3184f7455d5c4c300fac4a1c01fe72bee17543e61e67ab6108ef2f87104260c"
slug: "claude-model-selection-for-c--concerns"
path: "/home/zack/dev/llmx/claude_Model selection for current project_2026-01-17.md"
kind: "markdown"
lines: [740, 776]
token_estimate: 513
content_sha256: "5ca98c7db47101ea674a2d23d912ef39f9ee121c5a0dda935ac7a04becf9901d"
compacted: false
heading_path: ["Model selection for current project","Concerns"]
symbol: null
address: null
asset_path: null
---

## Concerns
1. Model size shows 133 MB but directions said ~33MB for bge-small. The file shows `models/bge-small-en-v1.5-opset13.onnx # Converted model (128 MB)` - that's the ONNX file, not the safetensors. At runtime they'll need the safetensors weights which are smaller.

2. The architecture looks good - SmartEmbeddingGenerator with automatic backend selection is exactly what we specified.

3. The agent correctly identified and solved the critical blockers (getrandom 0.3.3, tokenizers unstable_wasm, ONNX opset conversion).


4. The challenging part—getting Burn to compile to WASM—is complete. What remains is straightforward web development: fetching model weights, caching them in IndexedDB, and testing in the browser.Solid progress. The hard part is done—Burn compiles to WASM. Let me break down what I see:

**Completed (the tricky stuff):**
| Blocker | Solution | Status |
|---------|----------|--------|
| ONNX opset mismatch | Converted 11 → 13 | ✅ |
| tokenizers C deps | `unstable_wasm` feature | ✅ |
| getrandom WASM | v0.3.3 + `wasm_js` + `default-features = false` | ✅ |
| WASM build | `wasm-pack build --target web --release` works | ✅ |

**Output looks good:**
- WASM binary: **2.4 MB** (under 5MB target ✓)
- JS glue: 33 KB
- TypeScript defs included
- Fallback chain: WebGPU → CPU → Hash ✓

**One concern:** The summary says model weights are 133 MB but we spec'd ~33MB for bge-small. That 128MB is probably the ONNX file (which includes computation graph). The runtime safetensors should be smaller. Worth verifying.

**Remaining work (standard web dev):**
1. Fetch safetensors from HuggingFace CDN
2. IndexedDB caching
3. Forward pass (the pseudocode is there)
4. Mean pooling + L2 norm
5. Browser testing

The getrandom fix was the real blocker—glad they figured out it needs v0.3.3 specifically with `default-features = false`. That's a nasty edge case.

**Tell the agent:** Phase 6 compilation is done. Now complete the inference pipeline (priorities 1-3 in their "Next Phase" section) and verify the model weight size.