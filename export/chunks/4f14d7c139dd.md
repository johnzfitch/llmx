---
chunk_index: 711
ref: "4f14d7c139dd"
id: "4f14d7c139ddcb815f0c814e4952e3ea1e287c522a1d0252b53454f972527fe2"
slug: "post-p6-enhancements--3-splade-learned-sparse-retrieval"
path: "/home/zack/dev/llmx/docs/POST_P6_ENHANCEMENTS.md"
kind: "markdown"
lines: [91, 138]
token_estimate: 257
content_sha256: "86987e75f9284c051e1c66dcdc81ea8b104011245c66bff14870be96101a4884"
compacted: false
heading_path: ["Post-Phase 6 Enhancements","3. SPLADE (Learned Sparse Retrieval)"]
symbol: null
address: null
asset_path: null
---

## 3. SPLADE (Learned Sparse Retrieval)

**Problem:** BM25 is lexical-only. "authenticate" won't find "login".

**Problem with embeddings:** Slow (dot product with all chunks).

**SPLADE:** Best of both. Expands query into weighted terms, uses inverted index.

```
Query: "authentication"

BM25: {authentication: 1.0}

SPLADE: {
    authentication: 2.3,
    login: 1.8,
    verify: 1.2,
    credentials: 0.9,
    token: 0.7
}
```

**Implementation:**
```rust
pub struct SpladeModel {
    session: ort::Session,
    tokenizer: Tokenizer,
}

impl SpladeModel {
    pub fn expand(&self, query: &str) -> HashMap<String, f32> {
        let tokens = self.tokenizer.encode(query)?;
        let output = self.session.run([tokens])?;
        decode_sparse_vector(output)
    }
}

// In search
let expanded = splade.expand(&query);
let results = index.search_weighted(expanded, limit);
```

**Model:** `naver/splade-cocondenser-ensembledistil` (ONNX exportable, ~150MB)

**Impact:** 15-25% accuracy improvement, same speed as BM25.

---