---
chunk_index: 284
ref: "b22bb1ea18e3"
id: "b22bb1ea18e31be45f7f33a8b52d47f7c9ddf2135405f520631f59994c4837bc"
slug: "p6-directions--1-embedding-model-selection"
path: "/home/zack/dev/llmx/docs/P6_DIRECTIONS.md"
kind: "markdown"
lines: [10, 30]
token_estimate: 174
content_sha256: "5960d8b590656af24336a06c9f18ee6a266f9dec3b6f929790d37b053fff6afd"
compacted: false
heading_path: ["Phase 6: Burn + WebGPU Embeddings & Advanced Hybrid Search","1. Embedding Model Selection"]
symbol: null
address: null
asset_path: null
---

## 1. Embedding Model Selection

**Recommended: `bge-small-en-v1.5`** (drop-in replacement)
- 384-dim (matches current hash-based)
- Significantly outperforms MiniLM on MTEB
- BAAI/Alibaba, well-maintained
- ~33MB ONNX â†’ smaller after Burn compilation

**Alternative: `nomic-embed-text-v1.5`** (quality option)
- 768-dim native, supports Matryoshka (truncate to 256/384/512 at inference)
- Apache 2.0 license
- Better for code than general-purpose models
- ~137MB - may be too large for browser first-load

**Avoid:**
- `all-MiniLM-L6-v2` - dated (2021), superseded by above
- `ort` crate - doesn't compile to WASM
- Voyage/OpenAI APIs - requires server round-trip, defeats local-only goal

---