---
chunk_index: 48
ref: "cef87e219111"
id: "cef87e219111da4f87ac11bd20891b560cfe29b91dbc600c549dcde37226ed55"
slug: "claude-model-selection-for-c--assistant"
path: "/home/zack/dev/llmx/claude_Model selection for current project_2026-01-17.md"
kind: "markdown"
lines: [177, 225]
token_estimate: 559
content_sha256: "038250886b744b2cd72b29302b0a52c4dbee5e6f8abeaa5feced3175f2787d0d"
compacted: false
heading_path: ["Model selection for current project","Assistant"]
symbol: null
address: null
asset_path: null
---

## Assistant

The user uploaded LLMCAT_INTEGRATION_PLAN.md - let me read it to understand the current integration plan and see how it relates to the WebGPU/browser-only constraint.
Now I understand. llm.cat is browser-based with WebGPU for local processing. The current integration plan uses Python backend code, but Zack wants this to run entirely client-side with WebGPU.

The issue is:
1. P6 directions assume `ort` crate (Rust) which doesn't work in WASM
2. llm.cat needs browser-native embedding generation
3. WebGPU is the target runtime

I need to revise the P6 directions to work for the browser/WebGPU constraint. Options:
- **onnxruntime-web** - JavaScript, works with WebGPU backend
- **Transformers.js** - Hugging Face's JS port, uses onnxruntime-web under the hood
- **Burn with WGPU** - Rust → WASM, native WebGPU

For llm.cat specifically:
- The quality scoring (Layer E / specho-v2) is pure math - works fine in JS or Rust WASM
- Embeddings need ONNX inference - need to switch from `ort` to browser-compatible

Let me update the directions to reflect the WebGPU/browser constraint.Ah, that changes things. The P6 directions assume server-side `ort` which won't compile to WASM.

For browser + WebGPU, you need:

**Option 1: Transformers.js** (easiest)
- Hugging Face's JS port, uses onnxruntime-web with WebGPU backend
- Has `bge-small-en-v1.5` and `nomic-embed-text-v1.5` already converted
- ~5 lines of code to get embeddings

```javascript
import { pipeline } from '@xenova/transformers';
const embedder = await pipeline('feature-extraction', 'Xenova/bge-small-en-v1.5', {
  device: 'webgpu'
});
const embedding = await embedder('text to embed', { pooling: 'mean', normalize: true });
```

**Option 2: onnxruntime-web directly**
- More control, same backend as Transformers.js
- Need to handle tokenization yourself

**Option 3: Burn (Rust → WASM + WebGPU)**
- If you want to keep everything in Rust/WASM
- Compiles ONNX to Rust code, then to WASM
- More work but single stack

The quality scoring (Layer E) is pure math—works fine in JS or Rust WASM, no model needed.

Want me to rewrite P6 directions for browser/WebGPU? The core logic (RRF, hybrid ranking) stays the same, just the embedding generation changes.