---
chunk_index: 60
ref: "756b940ecc14"
id: "756b940ecc14b3790fae4141991f15fe60fc08b098362adc2350cb83823b5412"
slug: "claude-model-selection-for-c--assistant"
path: "/home/zack/dev/llmx/claude_Model selection for current project_2026-01-17.md"
kind: "markdown"
lines: [509, 554]
token_estimate: 751
content_sha256: "93d91a3ea2d4e416b3f772584446ae484bbaa56dbd20ed690b12bb4cf90c7517"
compacted: false
heading_path: ["Model selection for current project","Assistant"]
symbol: null
address: null
asset_path: null
---

## Assistant

The user is saying they haven't pushed Phase 5 yet, so I can't see the current state of the codebase from GitHub. They're pointing me to github.com/johnzfitch/llmx which would be the public repo, but it won't have the Phase 5 changes yet.

Let me think about what they're asking. The agent is asking whether to implement Phase 6 for:
1. Both native (desktop MCP server) and WASM (browser)
2. WASM/browser only

The question is about whether llmx and llm.cat share a codebase. Based on the conversation context:
- llmx is the MCP server (Rust, runs natively)
- llm.cat is a website (browser-based)

From the LLMCAT_INTEGRATION_PLAN.md I read earlier, it looks like llm.cat is a separate web application that would use quality filtering/scoring. The plan showed Python code for the backend (`backend/quality_filter.py`, `backend/exporter.py`), suggesting llm.cat might be a separate codebase.

But the user said "web gpu local only" earlier, which suggests llm.cat is meant to run entirely in the browser with WebGPU.

Given the ambiguity, I should give a practical recommendation:

1. If llm.cat is a separate frontend (likely a JS/TS web app), then the Rust/WASM embedding code would be a separate module that gets compiled to WASM and loaded by llm.cat.

2. The llmx MCP server is already working with Phase 5 hash-based embeddings for Claude Code integration.

The practical answer: **Option 2 (WASM/browser only)** makes sense for Phase 6 if the goal is getting real embeddings working in llm.cat. The MCP server can continue with hash-based embeddings until there's a need to upgrade it.

But actually, looking at this more carefully - if they want to eventually have the same embedding quality in both:


- Native MCP server (for Claude Code)
- Browser llm.cat

Then Option 1 with Burn makes sense because Burn abstracts the backend. The core embedding code is the same, just the weight loading differs. This approach lets me build once and deploy to both environments, which is cleaner long-term than maintaining separate implementations.Got it. So llmx is the Rust MCP server, and llm.cat is (presumably) a separate web frontend.

**Practical recommendation: Option 2 (WASM/browser only)**

Reasoning:
1. **llmx MCP server works fine** — Phase 5 hash-based embeddings are functional for Claude Code
2. **llm.cat is the priority** — that's where WebGPU/local-only matters
3. **Different deployment contexts** — MCP server runs natively with Claude, llm.cat runs in browser
4. **Avoid scope creep** — getting Burn + WebGPU working in browser is enough work

**Future option:** If you later want real embeddings in the native MCP server, Burn's backend abstraction makes it straightforward to add. The core code (tokenization, mean pooling, RRF) will be identical—just swap the weight loading path.

Tell the agent: **Option 2**

Once llm.cat has real semantic search working in browser, you can backport to native llmx if needed. The RRF and hybrid search logic will transfer directly.