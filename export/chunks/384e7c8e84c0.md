---
chunk_index: 353
ref: "384e7c8e84c0"
id: "384e7c8e84c0714765ed7970fdb02ba99247c4e22adae68fa718974cdab5f283"
slug: "phase6-fixes-completed--option-3-use-pre-tokenized-input-pattern"
path: "/home/zack/dev/llmx/docs/PHASE6_FIXES_COMPLETED.md"
kind: "markdown"
lines: [131, 151]
token_estimate: 150
content_sha256: "d796627512213de2f59e75acc3a2e41e170ec666730bf3f3e9d9ec13d86e5e9f"
compacted: false
heading_path: ["Phase 6 Blocker Fixes - Completion Report","Recommended Next Steps","Option 3: Use Pre-tokenized Input Pattern"]
symbol: null
address: null
asset_path: null
---

### Option 3: Use Pre-tokenized Input Pattern
Keep Burn for embeddings, but tokenize on JS side:

```javascript
// JS side
import { AutoTokenizer } from '@xenova/transformers';
const tokenizer = await AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5');
const { input_ids, attention_mask } = tokenizer(text);

// Pass to Rust
embedder.embed_tokenized(input_ids, attention_mask);
```

```rust
// Rust side - receives pre-tokenized input
#[wasm_bindgen]
pub fn embed_tokenized(&self, input_ids: Vec<i64>, attention_mask: Vec<i64>) -> Vec<f32> {
    // Skip tokenization, go straight to model
}
```