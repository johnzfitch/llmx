---
chunk_index: 1069
ref: "b45378c5e15f"
id: "b45378c5e15f1c9f23a79bca67fb0b834a029230506c647bc39b563268b31eb3"
slug: "embeddings-l1-126"
path: "/home/zack/dev/llmx/ingestor-core/src/embeddings.rs"
kind: "text"
lines: [1, 126]
token_estimate: 1005
content_sha256: "3d8ad2f9891492b1f8a515ce5478e0db8833e4c5792171f0cfa47fd72d78d54c"
compacted: false
heading_path: []
symbol: null
address: null
asset_path: null
---

/// Embedding generation for semantic search.
///
/// This module provides embedding functionality for code search.
/// Current implementation uses a simplified approach without requiring model files.
///
/// TODO Phase 5: Integrate real ONNX model (all-MiniLM-L6-v2)

use sha2::{Sha256, Digest};

/// Embedding dimension (matches all-MiniLM-L6-v2)
pub const EMBEDDING_DIM: usize = 384;

/// Compute cosine similarity between two normalized vectors.
///
/// Returns a value in [-1, 1], where 1 is identical.
pub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    if a.len() != b.len() {
        return 0.0;
    }
    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()
}

/// L2 normalize a vector.
pub fn normalize(vec: &[f32]) -> Vec<f32> {
    let norm = vec.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        vec.iter().map(|x| x / norm).collect()
    } else {
        vec.to_vec()
    }
}

/// Generate a deterministic embedding from text using a hash-based approach.
///
/// This is a placeholder implementation for testing the infrastructure.
///
/// TODO Phase 5: Replace with real ONNX-based embedding generation.
/// For production use:
/// 1. Download all-MiniLM-L6-v2 ONNX model from HuggingFace
/// 2. Add ort, tokenizers, ndarray dependencies (already in Cargo.toml)
/// 3. Implement proper tokenization and model inference
/// 4. Performance target: <50ms per chunk
pub fn generate_embedding(text: &str) -> Vec<f32> {
    // Use SHA-256 to generate deterministic pseudo-embeddings
    // This gives us consistent results for testing without requiring model files
    let mut hasher = Sha256::new();
    hasher.update(text.as_bytes());
    let hash = hasher.finalize();

    // Expand hash to embedding dimension
    let mut embedding = Vec::with_capacity(EMBEDDING_DIM);
    for i in 0..EMBEDDING_DIM {
        let idx = i % hash.len();
        let value = (hash[idx] as f32 - 128.0) / 128.0; // Normalize to [-1, 1]
        embedding.push(value);
    }

    // L2 normalize
    normalize(&embedding)
}

/// Generate embeddings for multiple texts.
///
/// Currently processes texts individually.
///
/// TODO Phase 5: Implement batch processing with ONNX for better performance.
pub fn generate_embeddings(texts: &[&str]) -> Vec<Vec<f32>> {
    texts.iter().map(|text| generate_embedding(text)).collect()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cosine_similarity() {
        let a = vec![1.0, 0.0, 0.0];
        let b = vec![1.0, 0.0, 0.0];
        assert!((cosine_similarity(&a, &b) - 1.0).abs() < 1e-6);

        let a = vec![1.0, 0.0, 0.0];
        let b = vec![0.0, 1.0, 0.0];
        assert!((cosine_similarity(&a, &b) - 0.0).abs() < 1e-6);
    }

    #[test]
    fn test_normalize() {
        let vec = vec![3.0, 4.0];
        let normalized = normalize(&vec);
        let norm = normalized.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!((norm - 1.0).abs() < 1e-6);
    }

    #[test]
    fn test_generate_embedding() {
        let text = "function hello() { return 'world'; }";
        let embedding = generate_embedding(text);
        assert_eq!(embedding.len(), EMBEDDING_DIM);

        // Check normalization
        let norm = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!((norm - 1.0).abs() < 1e-6);

        // Check determinism
        let embedding2 = generate_embedding(text);
        assert_eq!(embedding, embedding2);
    }

    #[test]
    fn test_similar_text_similarity() {
        let text1 = "const x = 42;";
        let text2 = "const x = 42;";
        let text3 = "function hello() {}";

        let emb1 = generate_embedding(text1);
        let emb2 = generate_embedding(text2);
        let emb3 = generate_embedding(text3);

        // Same text should have similarity 1.0
        assert!((cosine_similarity(&emb1, &emb2) - 1.0).abs() < 1e-6);

        // Different text should have lower similarity
        let sim13 = cosine_similarity(&emb1, &emb3);
        assert!(sim13 < 1.0);
    }
}