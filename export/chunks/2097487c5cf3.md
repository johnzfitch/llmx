---
chunk_index: 332
ref: "2097487c5cf3"
id: "2097487c5cf30424710390f07477927fd616f232b06babf217a6d16ec8c2971a"
slug: "phase6-blocker-fixes--solution-convert-to-opset-13"
path: "/home/zack/dev/llmx/docs/PHASE6_BLOCKER_FIXES.md"
kind: "markdown"
lines: [11, 103]
token_estimate: 557
content_sha256: "7e602f1be53b0cd55ba1ebe00f40594e37e15f2cc75b12c63a8172f37e9641a7"
compacted: false
heading_path: ["Phase 6 Blocker Resolution Guide","Blocker 1: ONNX Opset Mismatch","Solution: Convert to Opset 13"]
symbol: null
address: null
asset_path: null
---

### Solution: Convert to Opset 13

**Option A: Python conversion (recommended)**

```bash
# Install dependencies
pip install onnx onnxruntime

# Create conversion script
cat > convert_opset.py << 'EOF'
import onnx
from onnx import version_converter

# Load opset 11 model
model = onnx.load("models/bge-small-en-v1.5.onnx")
print(f"Original opset: {model.opset_import[0].version}")

# Convert to opset 13
converted = version_converter.convert_version(model, 13)
print(f"Converted opset: {converted.opset_import[0].version}")

# Validate
onnx.checker.check_model(converted)

# Save
onnx.save(converted, "models/bge-small-en-v1.5-opset13.onnx")
print("Saved: models/bge-small-en-v1.5-opset13.onnx")
EOF

python convert_opset.py
```

**Option B: Export fresh from PyTorch (if conversion fails)**

```bash
pip install transformers torch onnx

cat > export_model.py << 'EOF'
from transformers import AutoModel, AutoTokenizer
import torch

model_name = "BAAI/bge-small-en-v1.5"
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

model.eval()

# Dummy input
dummy = tokenizer("hello world", return_tensors="pt", padding=True, truncation=True)

# Export with opset 13
torch.onnx.export(
    model,
    (dummy["input_ids"], dummy["attention_mask"]),
    "models/bge-small-en-v1.5-opset13.onnx",
    input_names=["input_ids", "attention_mask"],
    output_names=["last_hidden_state"],
    dynamic_axes={
        "input_ids": {0: "batch", 1: "seq"},
        "attention_mask": {0: "batch", 1: "seq"},
        "last_hidden_state": {0: "batch", 1: "seq"}
    },
    opset_version=13,
    do_constant_folding=True
)
print("Exported with opset 13")
EOF

python export_model.py
```

**Update build.rs:**

```rust
// Change download URL or use local converted model
const MODEL_URL: &str = "file://models/bge-small-en-v1.5-opset13.onnx";
// Or host on your CDN after conversion
```

**Verification:**

```bash
# Check opset version
python -c "import onnx; m = onnx.load('models/bge-small-en-v1.5-opset13.onnx'); print(f'Opset: {m.opset_import[0].version}')"
# Should print: Opset: 13

# Test burn-import
cd ingestor-wasm && cargo build 2>&1 | grep -i opset
# Should not show opset errors
```

---