---
chunk_index: 289
ref: "889699392cca"
id: "889699392ccaeed04f3249bd08d78824a92acd01104c583b126dbc5c5589a427"
slug: "p6-directions--model-loading-runtime"
path: "/home/zack/dev/llmx/docs/P6_DIRECTIONS.md"
kind: "markdown"
lines: [75, 155]
token_estimate: 619
content_sha256: "4ce34256f2dfb09d02d9fe942326085b154861122b735f542e7dd0fa266ebfd1"
compacted: false
heading_path: ["Phase 6: Burn + WebGPU Embeddings & Advanced Hybrid Search","2. Burn Framework Integration","Model Loading (Runtime)"]
symbol: null
address: null
asset_path: null
---

### Model Loading (Runtime)
```rust
use burn::tensor::{Tensor, backend::Backend};
use burn_wgpu::{Wgpu, WgpuDevice};

// Generated by burn-import
mod model;
use model::BgeSmallModel;

pub struct EmbeddingGenerator<B: Backend> {
    model: BgeSmallModel<B>,
    tokenizer: Tokenizer,
    device: B::Device,
}

impl EmbeddingGenerator<Wgpu> {
    pub async fn new() -> Result<Self> {
        // WebGPU device (async in browser)
        let device = WgpuDevice::default();
        
        // Load model weights (fetch from CDN or IndexedDB cache)
        let weights = load_weights().await?;
        let model = BgeSmallModel::new(&device, weights);
        
        // Tokenizer (bundled or fetched)
        let tokenizer = Tokenizer::from_bytes(include_bytes!("tokenizer.json"))?;
        
        Ok(Self { model, tokenizer, device })
    }
    
    pub fn embed(&self, text: &str) -> Vec<f32> {
        // Tokenize
        let encoding = self.tokenizer.encode(text, true).unwrap();
        let input_ids: Vec<i64> = encoding.get_ids()
            .iter()
            .take(512)  // Max length
            .map(|&id| id as i64)
            .collect();
        let attention_mask: Vec<f32> = encoding.get_attention_mask()
            .iter()
            .take(512)
            .map(|&m| m as f32)
            .collect();
        
        // To tensors
        let input_tensor = Tensor::<Wgpu, 2, Int>::from_data(
            &[input_ids.as_slice()], 
            &self.device
        );
        let mask_tensor = Tensor::<Wgpu, 2>::from_data(
            &[attention_mask.as_slice()],
            &self.device
        );
        
        // Forward pass
        let hidden_states = self.model.forward(input_tensor, mask_tensor);
        
        // Mean pooling with attention mask
        let pooled = self.mean_pool(hidden_states, mask_tensor);
        
        // L2 normalize
        let normalized = pooled.normalize(2, 1);
        
        // To Vec<f32>
        normalized.into_data().value
    }
    
    fn mean_pool(
        &self, 
        hidden: Tensor<Wgpu, 3>,  // [batch, seq, dim]
        mask: Tensor<Wgpu, 2>     // [batch, seq]
    ) -> Tensor<Wgpu, 2> {        // [batch, dim]
        let mask_expanded = mask.unsqueeze_dim(2);  // [batch, seq, 1]
        let masked = hidden * mask_expanded;
        let summed = masked.sum_dim(1);             // [batch, dim]
        let counts = mask.sum_dim(1).unsqueeze_dim(1);
        summed / counts
    }
}
```