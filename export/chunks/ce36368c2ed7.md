---
chunk_index: 335
ref: "ce36368c2ed7"
id: "ce36368c2ed767f2746c5c0baa8b5f619ed34eef008cd84a5f82825c4b5338af"
slug: "phase6-blocker-fixes--solution-b-pure-rust-wordpiece-fallback"
path: "/home/zack/dev/llmx/docs/PHASE6_BLOCKER_FIXES.md"
kind: "markdown"
lines: [142, 233]
token_estimate: 641
content_sha256: "6a16d1f227d26cb52ddd1fe43050e6356d544d0200f57345ca3d215b9f2bc0eb"
compacted: false
heading_path: ["Phase 6 Blocker Resolution Guide","Blocker 2: Tokenizer WASM Incompatibility","Solution B: Pure Rust WordPiece (Fallback)"]
symbol: null
address: null
asset_path: null
---

### Solution B: Pure Rust WordPiece (Fallback)

If `tokenizers` still fails, implement minimal WordPiece:

```rust
use std::collections::HashMap;

pub struct SimpleWordPieceTokenizer {
    vocab: HashMap<String, u32>,
    unk_token_id: u32,
    cls_token_id: u32,
    sep_token_id: u32,
    pad_token_id: u32,
}

impl SimpleWordPieceTokenizer {
    pub fn from_vocab_json(json_bytes: &[u8]) -> Self {
        let vocab: HashMap<String, u32> = serde_json::from_slice(json_bytes).unwrap();
        Self {
            unk_token_id: vocab["[UNK]"],
            cls_token_id: vocab["[CLS]"],
            sep_token_id: vocab["[SEP]"],
            pad_token_id: vocab["[PAD]"],
            vocab,
        }
    }

    pub fn encode(&self, text: &str, max_length: usize) -> (Vec<u32>, Vec<u32>) {
        let mut input_ids = vec![self.cls_token_id];
        let mut attention_mask = vec![1u32];

        // Simple whitespace + lowercase tokenization
        for word in text.to_lowercase().split_whitespace() {
            let token_id = self.tokenize_word(word);
            if input_ids.len() < max_length - 1 {
                input_ids.push(token_id);
                attention_mask.push(1);
            }
        }

        // Add [SEP]
        input_ids.push(self.sep_token_id);
        attention_mask.push(1);

        // Pad to max_length
        while input_ids.len() < max_length {
            input_ids.push(self.pad_token_id);
            attention_mask.push(0);
        }

        (input_ids, attention_mask)
    }

    fn tokenize_word(&self, word: &str) -> u32 {
        // Try whole word first
        if let Some(&id) = self.vocab.get(word) {
            return id;
        }

        // Try WordPiece subwords
        let mut start = 0;
        while start < word.len() {
            let mut end = word.len();
            let mut found = false;

            while start < end {
                let substr = if start == 0 {
                    &word[start..end]
                } else {
                    &format!("##{}", &word[start..end])
                };

                if self.vocab.contains_key(substr) {
                    found = true;
                    break;
                }
                end -= 1;
            }

            if !found {
                return self.unk_token_id;
            }
            start = end;
        }

        self.unk_token_id // Simplified - real impl would collect subword IDs
    }
}
```

**Note:** This is a simplified implementation. For production, use the `tokenizers` crate with WASM features.