---
chunk_index: 62
ref: "1acf6f7310a2"
id: "1acf6f7310a2c50f1c5c8e50849ccce4c8eb8b8ab9bffdc5c5f84f8a20db0490"
slug: "claude-model-selection-for-c--assistant"
path: "/home/zack/dev/llmx/claude_Model selection for current project_2026-01-17.md"
kind: "markdown"
lines: [559, 590]
token_estimate: 361
content_sha256: "2a3f6e1ca69aedaa77a20fc850ba9d387771b16c0314fd6657f968a3d178999b"
compacted: false
heading_path: ["Model selection for current project","Assistant"]
symbol: null
address: null
asset_path: null
---

## Assistant

The user is clarifying their vision for llm.cat - it's a browser-based tool where users:
1. Drag/drop files into browser
2. Get instant processing (chunking, indexing, embeddings) locally via WebGPU
3. Download a zip with the results (llms.txt, index, etc.)
4. Put it wherever they need it

This is a simpler, cleaner UX than running an MCP server. The processing happens entirely client-side in the browser, no server needed.

This confirms Option 2 is correct - focus on WASM/browser. The MCP server (llmx) is a separate tool for different use cases (Claude Code integration).

Let me confirm this understanding and validate the approach.Exactly right. That's the clean UX:

1. **Open llm.cat in browser**
2. **Drag folder** → WebGPU crunches locally (chunking, BM25 index, embeddings)
3. **Download zip** → `llms.txt` + `index.json` (with embeddings baked in)
4. **Use anywhere** — drop into project, feed to LLM, whatever

No install. No server. No API keys. No data leaving your machine.

The MCP server (llmx) is a different tool for a different context (Claude Code integration where you need live search during a session). llm.cat is a generator/exporter.

**So yes: Option 2, browser-only.**

The agent should focus on:
- Burn + WebGPU in browser
- IndexedDB for model weight caching
- Clean export format (zip with index + embeddings)

Native MCP server stays on Phase 5 hash-based until there's a reason to upgrade it.