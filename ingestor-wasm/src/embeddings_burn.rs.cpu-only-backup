/// Phase 6: Burn-based embeddings with WebGPU acceleration
///
/// Provides real semantic embeddings using arctic-embed-s model
/// running on WebGPU in the browser.

use burn::tensor::{backend::Backend, Int, Tensor, TensorData};
#[cfg(feature = "ndarray-backend")]
use burn_ndarray::{NdArray, NdArrayDevice};
#[cfg(feature = "wgpu-backend")]
use burn_wgpu::{Wgpu, WgpuDevice};
use crate::model_loader::{fetch_with_cache, Model, MODEL_ID};
#[cfg(feature = "wgpu-backend")]
use crate::model_loader::load_model;
#[cfg(feature = "ndarray-backend")]
use crate::model_loader::load_model_cpu;
use tokenizers::Tokenizer;
use wasm_bindgen::prelude::*;

/// Embedding dimension for arctic-embed-s
pub const EMBEDDING_DIM: usize = 384;

/// Maximum sequence length for the model
const MAX_SEQ_LENGTH: usize = 512;

/// Tokenizer URL (prefer same-origin to avoid CORS / third-party outages).
const TOKENIZER_URL_PRIMARY: &str = "./models/tokenizer.json";
const TOKENIZER_URL_FALLBACK: &str =
    "https://huggingface.co/Snowflake/snowflake-arctic-embed-s/resolve/main/tokenizer.json";
const TOKENIZER_CACHE_KEY: &str = "arctic-embed-s-tokenizer-v1";
const TOKENIZER_SHA256: &str = "91f1def9b9391fdabe028cd3f3fcc4efd34e5d1f08c3bf2de513ebb5911a1854";
const MAX_TOKENIZER_BYTES: usize = 5 * 1024 * 1024;
const ALLOWED_TOKENIZER_ORIGINS: [&str; 1] = ["https://huggingface.co/"];

/// Backend-agnostic embedding generator trait
pub trait EmbeddingBackend {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue>;
    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue>;
    fn dimension(&self) -> usize;
}

/// WebGPU-accelerated embedding generator
#[cfg(feature = "wgpu-backend")]
pub struct WgpuEmbeddingGenerator {
    model: Model<Wgpu>,
    tokenizer: Tokenizer,
    device: WgpuDevice,
}

#[cfg(feature = "wgpu-backend")]
impl WgpuEmbeddingGenerator {
    /// Initialize WebGPU embedding generator
    pub async fn new() -> Result<Self, JsValue> {
        web_sys::console::log_1(&JsValue::from_str("WebGPU init: creating WgpuDevice..."));

        // Initialize WebGPU device
        let device = WgpuDevice::default();

        web_sys::console::log_1(&JsValue::from_str("WebGPU init: loading tokenizer..."));

        let tokenizer_bytes = fetch_tokenizer_bytes().await?;
        let tokenizer = Tokenizer::from_bytes(&tokenizer_bytes)
            .map_err(|e| {
                web_sys::console::error_1(&JsValue::from_str(&format!(
                    "Tokenizer load failed: {e}"
                )));
                JsValue::from_str("Failed to load tokenizer")
            })?;

        web_sys::console::log_1(&JsValue::from_str("WebGPU init: loading model..."));
        let model = load_model(&device).await?;

        web_sys::console::log_1(&JsValue::from_str("WebGPU init: embedder ready"));
        Ok(Self { model, tokenizer, device })
    }
}

#[cfg(feature = "wgpu-backend")]
impl EmbeddingBackend for WgpuEmbeddingGenerator {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        embed_with_model(&self.model, &self.tokenizer, &self.device, text)
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        embed_batch_with_model(&self.model, &self.tokenizer, &self.device, texts)
    }

    fn dimension(&self) -> usize {
        EMBEDDING_DIM
    }
}

/// CPU fallback embedding generator (NdArray backend)
#[cfg(feature = "ndarray-backend")]
pub struct CpuEmbeddingGenerator {
    model: Model<NdArray>,
    tokenizer: Tokenizer,
    device: NdArrayDevice,
}

#[cfg(feature = "ndarray-backend")]
impl CpuEmbeddingGenerator {
    pub async fn new() -> Result<Self, JsValue> {
        let device = NdArrayDevice::default();

        let tokenizer_bytes = fetch_tokenizer_bytes().await?;
        let tokenizer = Tokenizer::from_bytes(&tokenizer_bytes)
            .map_err(|e| {
                web_sys::console::error_1(&JsValue::from_str(&format!(
                    "Tokenizer load failed: {e}"
                )));
                JsValue::from_str("Failed to load tokenizer")
            })?;

        let model = load_model_cpu(&device).await?;

        Ok(Self {
            model,
            tokenizer,
            device,
        })
    }
}

#[cfg(feature = "ndarray-backend")]
impl EmbeddingBackend for CpuEmbeddingGenerator {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        embed_with_model(&self.model, &self.tokenizer, &self.device, text)
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        embed_batch_with_model(&self.model, &self.tokenizer, &self.device, texts)
    }

    fn dimension(&self) -> usize {
        EMBEDDING_DIM
    }
}

/// Hash-based fallback (Phase 5 compatibility)
pub struct HashEmbeddingGenerator;

impl HashEmbeddingGenerator {
    pub fn new() -> Self {
        Self
    }
}

impl EmbeddingBackend for HashEmbeddingGenerator {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        // Use the existing hash-based embedding from ingestor-core
        // This is the Phase 5 fallback
        use sha2::{Digest, Sha256};

        let mut hasher = Sha256::new();
        hasher.update(text.as_bytes());
        let hash = hasher.finalize();

        let mut embedding = Vec::with_capacity(EMBEDDING_DIM);
        for i in 0..EMBEDDING_DIM {
            let idx = i % hash.len();
            let value = (hash[idx] as f32 - 128.0) / 128.0;
            embedding.push(value);
        }

        // L2 normalize
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > 0.0 {
            for x in &mut embedding {
                *x /= norm;
            }
        }

        Ok(embedding)
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        texts.iter()
            .map(|text| self.embed(text))
            .collect()
    }

    fn dimension(&self) -> usize {
        EMBEDDING_DIM
    }
}

/// Smart embedding generator with automatic fallback
pub enum SmartEmbeddingGenerator {
    #[cfg(feature = "wgpu-backend")]
    WebGpu(WgpuEmbeddingGenerator),
    #[cfg(feature = "ndarray-backend")]
    Cpu(CpuEmbeddingGenerator),
    Hash(HashEmbeddingGenerator),
}

impl SmartEmbeddingGenerator {
    /// Create embedding generator with automatic fallback chain:
    /// WebGPU → CPU → Hash-based
    pub async fn new() -> Self {
        // TEMPORARY: Skip WebGPU in WASM due to Burn 0.21 WGPU panic during model loading
        // Issue: recorder.load() triggers "unreachable executed" panic in WASM
        // Native Rust works fine, WASM-specific bug
        // TODO: Re-enable when Burn 0.22+ fixes WGPU WASM compatibility
        // Uncomment the block below to re-enable WebGPU:

        /*
        #[cfg(feature = "wgpu-backend")]
        {
            web_sys::console::log_1(&JsValue::from_str("Attempting WebGPU initialization..."));
            match WgpuEmbeddingGenerator::new().await {
                Ok(gen) => {
                    web_sys::console::log_1(&JsValue::from_str("Embeddings backend: WebGPU"));
                    return Self::WebGpu(gen);
                }
                Err(e) => {
                    web_sys::console::error_1(&JsValue::from_str(&format!(
                        "WebGPU initialization failed: {:?}",
                        e
                    )));
                }
            }
        }
        */

        // Fall back to CPU (primary backend for WASM until bug is fixed)
        #[cfg(feature = "ndarray-backend")]
        if let Ok(gen) = CpuEmbeddingGenerator::new().await {
            web_sys::console::log_1(&JsValue::from_str("Embeddings backend: CPU"));
            return Self::Cpu(gen);
        }

        // Last resort: hash-based
        web_sys::console::log_1(&JsValue::from_str("Embeddings backend: hash (models unavailable)"));
        Self::Hash(HashEmbeddingGenerator::new())
    }
}

impl EmbeddingBackend for SmartEmbeddingGenerator {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        match self {
            #[cfg(feature = "wgpu-backend")]
            Self::WebGpu(gen) => gen.embed(text),
            #[cfg(feature = "ndarray-backend")]
            Self::Cpu(gen) => gen.embed(text),
            Self::Hash(gen) => gen.embed(text),
        }
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        match self {
            #[cfg(feature = "wgpu-backend")]
            Self::WebGpu(gen) => gen.embed_batch(texts),
            #[cfg(feature = "ndarray-backend")]
            Self::Cpu(gen) => gen.embed_batch(texts),
            Self::Hash(gen) => gen.embed_batch(texts),
        }
    }

    fn dimension(&self) -> usize {
        match self {
            #[cfg(feature = "wgpu-backend")]
            Self::WebGpu(gen) => gen.dimension(),
            #[cfg(feature = "ndarray-backend")]
            Self::Cpu(gen) => gen.dimension(),
            Self::Hash(gen) => gen.dimension(),
        }
    }
}

fn embed_with_model<B: Backend>(
    model: &Model<B>,
    tokenizer: &Tokenizer,
    device: &B::Device,
    text: &str,
) -> Result<Vec<f32>, JsValue> {
    let encoding = tokenizer
        .encode(text, true)
        .map_err(|e| {
            web_sys::console::error_1(&JsValue::from_str(&format!("Tokenization failed: {e}")));
            JsValue::from_str("Failed to tokenize input")
        })?;

    let input_ids: Vec<i64> = encoding
        .get_ids()
        .iter()
        .take(MAX_SEQ_LENGTH)
        .map(|&id| id as i64)
        .collect();

    let attention_mask: Vec<i64> = encoding
        .get_attention_mask()
        .iter()
        .take(MAX_SEQ_LENGTH)
        .map(|&mask| mask as i64)
        .collect();

    if input_ids.is_empty() {
        return Err(JsValue::from_str("Tokenization produced no input ids"));
    }

    let seq_len = input_ids.len();
    let input_ids = Tensor::<B, 2, Int>::from_ints(
        TensorData::new(input_ids, [1, seq_len]),
        device,
    );
    let attention_mask = Tensor::<B, 2, Int>::from_ints(
        TensorData::new(attention_mask, [1, seq_len]),
        device,
    );

    let hidden = model.forward(input_ids, attention_mask.clone());
    let pooled = mean_pool(hidden, attention_mask);
    let normalized = l2_normalize(pooled);

    let data = normalized.into_data();
    data.to_vec::<f32>()
        .map_err(|err| {
            web_sys::console::error_1(&JsValue::from_str(&format!(
                "Embedding read failed: {err:?}"
            )));
            JsValue::from_str("Failed to read embedding")
        })
}

fn embed_batch_with_model<B: Backend>(
    model: &Model<B>,
    tokenizer: &Tokenizer,
    device: &B::Device,
    texts: &[String],
) -> Result<Vec<Vec<f32>>, JsValue> {
    if texts.is_empty() {
        return Ok(Vec::new());
    }

    let mut encoded_ids: Vec<Vec<i64>> = Vec::with_capacity(texts.len());
    let mut encoded_masks: Vec<Vec<i64>> = Vec::with_capacity(texts.len());
    let mut max_len = 0usize;

    for text in texts {
        let encoding = tokenizer
            .encode(text.as_str(), true)
            .map_err(|e| {
                web_sys::console::error_1(&JsValue::from_str(&format!(
                    "Tokenization failed: {e}"
                )));
                JsValue::from_str("Failed to tokenize input")
            })?;

        let ids: Vec<i64> = encoding
            .get_ids()
            .iter()
            .take(MAX_SEQ_LENGTH)
            .map(|&id| id as i64)
            .collect();
        let mask: Vec<i64> = encoding
            .get_attention_mask()
            .iter()
            .take(MAX_SEQ_LENGTH)
            .map(|&mask| mask as i64)
            .collect();

        if ids.is_empty() {
            return Err(JsValue::from_str("Tokenization produced no input ids"));
        }

        max_len = max_len.max(ids.len());
        encoded_ids.push(ids);
        encoded_masks.push(mask);
    }

    let batch_size = encoded_ids.len();
    let mut flat_ids: Vec<i64> = Vec::with_capacity(batch_size * max_len);
    let mut flat_masks: Vec<i64> = Vec::with_capacity(batch_size * max_len);

    for (mut ids, mut masks) in encoded_ids.into_iter().zip(encoded_masks) {
        ids.resize(max_len, 0);
        masks.resize(max_len, 0);
        flat_ids.extend_from_slice(&ids);
        flat_masks.extend_from_slice(&masks);
    }

    let input_ids = Tensor::<B, 2, Int>::from_ints(
        TensorData::new(flat_ids, [batch_size, max_len]),
        device,
    );
    let attention_mask = Tensor::<B, 2, Int>::from_ints(
        TensorData::new(flat_masks, [batch_size, max_len]),
        device,
    );

    let hidden = model.forward(input_ids, attention_mask.clone());
    let pooled = mean_pool(hidden, attention_mask);
    let normalized = l2_normalize(pooled);

    let data = normalized.into_data();
    let flat = data.to_vec::<f32>().map_err(|err| {
        web_sys::console::error_1(&JsValue::from_str(&format!(
            "Embedding batch read failed: {err:?}"
        )));
        JsValue::from_str("Failed to read embeddings")
    })?;

    if flat.len() != batch_size * EMBEDDING_DIM {
        return Err(JsValue::from_str("Embedding batch size mismatch"));
    }

    let mut outputs = Vec::with_capacity(batch_size);
    for chunk in flat.chunks(EMBEDDING_DIM) {
        outputs.push(chunk.to_vec());
    }

    Ok(outputs)
}

fn mean_pool<B: Backend>(hidden: Tensor<B, 3>, attention_mask: Tensor<B, 2, Int>) -> Tensor<B, 2> {
    let mask = attention_mask.float().unsqueeze_dim::<3>(2);
    let masked = hidden * mask.clone();
    let sum = masked.sum_dim(1);
    let denom = mask.sum_dim(1).clamp_min(1e-6);
    let pooled = sum / denom;
    pooled.squeeze_dim::<2>(1)
}

fn l2_normalize<B: Backend>(embeddings: Tensor<B, 2>) -> Tensor<B, 2> {
    let norm = embeddings
        .clone()
        .powf_scalar(2.0)
        .sum_dim(1)
        .sqrt()
        .clamp_min(1e-12);
    embeddings / norm
}

/// Fetch resource from CDN and cache in IndexedDB
async fn load_or_fetch_from_cdn(url: &str, cache_key: &str) -> Result<Vec<u8>, JsValue> {
    fetch_with_cache(
        url,
        cache_key,
        TOKENIZER_SHA256,
        &ALLOWED_TOKENIZER_ORIGINS,
        MAX_TOKENIZER_BYTES,
    )
    .await
}

async fn fetch_tokenizer_bytes() -> Result<Vec<u8>, JsValue> {
    match load_or_fetch_from_cdn(TOKENIZER_URL_PRIMARY, TOKENIZER_CACHE_KEY).await {
        Ok(bytes) => Ok(bytes),
        Err(primary_err) => {
            web_sys::console::warn_1(&JsValue::from_str(&format!(
                "Tokenizer primary fetch failed, falling back: {primary_err:?}"
            )));
            load_or_fetch_from_cdn(TOKENIZER_URL_FALLBACK, TOKENIZER_CACHE_KEY).await
        }
    }
}

/// WASM bindings for browser
#[wasm_bindgen]
pub struct Embedder {
    inner: SmartEmbeddingGenerator,
}

#[wasm_bindgen]
impl Embedder {
    /// Create new embedder with automatic backend selection
    ///
    /// Use this factory method instead of `new()` to avoid TypeScript issues
    /// with async constructors.
    ///
    /// # Example
    /// ```javascript
    /// const embedder = await Embedder.create();
    /// ```
    #[wasm_bindgen]
    pub async fn create() -> Result<Embedder, JsValue> {
        let inner = SmartEmbeddingGenerator::new().await;
        Ok(Embedder { inner })
    }

    /// Generate embedding for a single text
    #[wasm_bindgen]
    pub fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        self.inner.embed(text)
    }

    /// Generate embeddings for multiple texts
    #[wasm_bindgen(js_name = embedBatch)]
    pub fn embed_batch(&self, texts: Vec<String>) -> Result<JsValue, JsValue> {
        let embeddings = self.inner.embed_batch(&texts)?;

        // Convert Vec<Vec<f32>> to JS array
        let js_array = js_sys::Array::new();
        for embedding in embeddings {
            let js_vec = js_sys::Float32Array::from(&embedding[..]);
            js_array.push(&js_vec);
        }

        Ok(js_array.into())
    }

    /// Get embedding dimension
    #[wasm_bindgen]
    pub fn dimension(&self) -> usize {
        self.inner.dimension()
    }

    /// Get model identifier
    #[wasm_bindgen(js_name = modelId)]
    pub fn model_id(&self) -> String {
        match &self.inner {
            #[cfg(feature = "wgpu-backend")]
            SmartEmbeddingGenerator::WebGpu(_) => MODEL_ID.to_string(),
            #[cfg(feature = "ndarray-backend")]
            SmartEmbeddingGenerator::Cpu(_) => MODEL_ID.to_string(),
            SmartEmbeddingGenerator::Hash(_) => "hash-based-v1".to_string(),
        }
    }
}
