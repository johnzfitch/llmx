/// Phase 6: Burn-based embeddings with WebGPU acceleration
///
/// Provides real semantic embeddings using bge-small-en-v1.5 model
/// running on WebGPU in the browser.

use burn::tensor::{backend::Backend, Tensor};
use burn_ndarray::NdArrayDevice;
use burn_wgpu::WgpuDevice;
use tokenizers::Tokenizer;
use wasm_bindgen::prelude::*;
use wasm_bindgen_futures::JsFuture;
use web_sys::{Request, RequestInit, RequestMode, Response};

// Generated by build.rs from ONNX model
// This will be created after first build
// mod model;

/// Embedding dimension for bge-small-en-v1.5
pub const BGE_EMBEDDING_DIM: usize = 384;

/// Maximum sequence length for the model
const MAX_SEQ_LENGTH: usize = 512;

/// Model identifier for caching
pub const BGE_MODEL_ID: &str = "bge-small-en-v1.5";

/// CDN URL for model weights (to be fetched at runtime)
const MODEL_WEIGHTS_URL: &str =
    "https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/model.safetensors";

/// CDN URL for tokenizer
const TOKENIZER_URL: &str =
    "https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.json";

/// Backend-agnostic embedding generator trait
pub trait EmbeddingBackend: Send {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue>;
    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue>;
    fn dimension(&self) -> usize;
}

/// WebGPU-accelerated embedding generator
pub struct WgpuEmbeddingGenerator {
    // model: model::BgeSmallModel<Wgpu>,
    tokenizer: Tokenizer,
    device: WgpuDevice,
}

impl WgpuEmbeddingGenerator {
    /// Initialize WebGPU embedding generator
    pub async fn new() -> Result<Self, JsValue> {
        // Initialize WebGPU device
        let device = WgpuDevice::default();

        // Load tokenizer from CDN or cache
        let tokenizer_bytes = load_or_fetch_from_cdn(TOKENIZER_URL, "bge-tokenizer").await?;
        let tokenizer = Tokenizer::from_bytes(&tokenizer_bytes)
            .map_err(|e| JsValue::from_str(&format!("Failed to load tokenizer: {}", e)))?;

        // Load model weights from CDN or IndexedDB cache
        // TODO: Implement after model code generation
        // let weights_bytes = load_or_fetch_from_cdn(MODEL_WEIGHTS_URL, "bge-weights").await?;
        // let model = model::BgeSmallModel::load(&device, &weights_bytes)?;

        Ok(Self {
            // model,
            tokenizer,
            device,
        })
    }

    #[allow(dead_code)]
    fn mean_pool<B: Backend>(
        &self,
        _hidden: Tensor<B, 3>, // [batch, seq, dim]
        _mask: Tensor<B, 2>,   // [batch, seq]
    ) -> Tensor<B, 2> {
        // TODO: This will be properly implemented when the model is integrated
        // For now, this is a placeholder that won't be called
        panic!("mean_pool not yet implemented - requires model integration")
    }
}

impl EmbeddingBackend for WgpuEmbeddingGenerator {
    fn embed(&self, _text: &str) -> Result<Vec<f32>, JsValue> {
        // TODO: Implement after model code generation
        // For now, return error
        Err(JsValue::from_str("WebGPU embeddings not yet implemented - model code generation pending"))

        /* Implementation will look like this:
        let encoding = self.tokenizer
            .encode(text, true)
            .map_err(|e| JsValue::from_str(&format!("Tokenization failed: {}", e)))?;

        let input_ids: Vec<i64> = encoding
            .get_ids()
            .iter()
            .take(MAX_SEQ_LENGTH)
            .map(|&id| id as i64)
            .collect();

        let attention_mask: Vec<f32> = encoding
            .get_attention_mask()
            .iter()
            .take(MAX_SEQ_LENGTH)
            .map(|&m| m as f32)
            .collect();

        // Convert to tensors
        let input_tensor = Tensor::<Wgpu, 1, Int>::from_data(
            Data::new(input_ids, [input_ids.len()].into()),
            &self.device,
        ).unsqueeze::<2>();

        let mask_tensor = Tensor::<Wgpu, 1>::from_data(
            Data::new(attention_mask, [attention_mask.len()].into()),
            &self.device,
        ).unsqueeze::<2>();

        // Forward pass
        let hidden_states = self.model.forward(input_tensor, mask_tensor);

        // Mean pooling with attention mask
        let pooled = self.mean_pool(hidden_states, mask_tensor.squeeze(0));

        // L2 normalize
        let normalized = pooled.clone() / pooled.clone().powf_scalar(2.0).sum_dim(1).sqrt().unsqueeze();

        // Convert to Vec<f32>
        let data: Data<f32, 1> = normalized.into_data();
        Ok(data.value)
        */
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        // For now, process sequentially
        // TODO: Implement true batching after model code generation
        texts.iter()
            .map(|text| self.embed(text))
            .collect()
    }

    fn dimension(&self) -> usize {
        BGE_EMBEDDING_DIM
    }
}

/// CPU fallback embedding generator (NdArray backend)
pub struct CpuEmbeddingGenerator {
    // model: model::BgeSmallModel<NdArray>,
    tokenizer: Tokenizer,
    device: NdArrayDevice,
}

impl CpuEmbeddingGenerator {
    pub async fn new() -> Result<Self, JsValue> {
        let device = NdArrayDevice::default();

        // Load tokenizer from CDN or cache
        let tokenizer_bytes = load_or_fetch_from_cdn(TOKENIZER_URL, "bge-tokenizer").await?;
        let tokenizer = Tokenizer::from_bytes(&tokenizer_bytes)
            .map_err(|e| JsValue::from_str(&format!("Failed to load tokenizer: {}", e)))?;

        // TODO: Load model weights

        Ok(Self {
            // model,
            tokenizer,
            device,
        })
    }
}

impl EmbeddingBackend for CpuEmbeddingGenerator {
    fn embed(&self, _text: &str) -> Result<Vec<f32>, JsValue> {
        Err(JsValue::from_str("CPU embeddings not yet implemented"))
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        texts.iter()
            .map(|text| self.embed(text))
            .collect()
    }

    fn dimension(&self) -> usize {
        BGE_EMBEDDING_DIM
    }
}

/// Hash-based fallback (Phase 5 compatibility)
pub struct HashEmbeddingGenerator;

impl HashEmbeddingGenerator {
    pub fn new() -> Self {
        Self
    }
}

impl EmbeddingBackend for HashEmbeddingGenerator {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        // Use the existing hash-based embedding from ingestor-core
        // This is the Phase 5 fallback
        use sha2::{Digest, Sha256};

        let mut hasher = Sha256::new();
        hasher.update(text.as_bytes());
        let hash = hasher.finalize();

        let mut embedding = Vec::with_capacity(BGE_EMBEDDING_DIM);
        for i in 0..BGE_EMBEDDING_DIM {
            let idx = i % hash.len();
            let value = (hash[idx] as f32 - 128.0) / 128.0;
            embedding.push(value);
        }

        // L2 normalize
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > 0.0 {
            for x in &mut embedding {
                *x /= norm;
            }
        }

        Ok(embedding)
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        texts.iter()
            .map(|text| self.embed(text))
            .collect()
    }

    fn dimension(&self) -> usize {
        BGE_EMBEDDING_DIM
    }
}

/// Smart embedding generator with automatic fallback
pub enum SmartEmbeddingGenerator {
    WebGpu(WgpuEmbeddingGenerator),
    Cpu(CpuEmbeddingGenerator),
    Hash(HashEmbeddingGenerator),
}

impl SmartEmbeddingGenerator {
    /// Create embedding generator with automatic fallback chain:
    /// WebGPU → CPU → Hash-based
    pub async fn new() -> Self {
        // Try WebGPU first
        if let Ok(gen) = WgpuEmbeddingGenerator::new().await {
            web_sys::console::log_1(&JsValue::from_str("Using WebGPU embeddings"));
            return Self::WebGpu(gen);
        }

        // Fall back to CPU
        if let Ok(gen) = CpuEmbeddingGenerator::new().await {
            web_sys::console::log_1(&JsValue::from_str("Using CPU embeddings (WebGPU unavailable)"));
            return Self::Cpu(gen);
        }

        // Last resort: hash-based
        web_sys::console::log_1(&JsValue::from_str("Using hash-based embeddings (models unavailable)"));
        Self::Hash(HashEmbeddingGenerator::new())
    }
}

impl EmbeddingBackend for SmartEmbeddingGenerator {
    fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        match self {
            Self::WebGpu(gen) => gen.embed(text),
            Self::Cpu(gen) => gen.embed(text),
            Self::Hash(gen) => gen.embed(text),
        }
    }

    fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, JsValue> {
        match self {
            Self::WebGpu(gen) => gen.embed_batch(texts),
            Self::Cpu(gen) => gen.embed_batch(texts),
            Self::Hash(gen) => gen.embed_batch(texts),
        }
    }

    fn dimension(&self) -> usize {
        match self {
            Self::WebGpu(gen) => gen.dimension(),
            Self::Cpu(gen) => gen.dimension(),
            Self::Hash(gen) => gen.dimension(),
        }
    }
}

/// Fetch resource from CDN and cache in IndexedDB
async fn load_or_fetch_from_cdn(url: &str, _cache_key: &str) -> Result<Vec<u8>, JsValue> {
    // TODO: Check IndexedDB cache first
    // For now, just fetch from CDN

    let opts = RequestInit::new();
    opts.set_method("GET");
    opts.set_mode(RequestMode::Cors);

    let request = Request::new_with_str_and_init(url, &opts)?;

    let window = web_sys::window().ok_or_else(|| JsValue::from_str("No window object"))?;
    let resp_value = JsFuture::from(window.fetch_with_request(&request)).await?;
    let resp: Response = resp_value.dyn_into()?;

    if !resp.ok() {
        return Err(JsValue::from_str(&format!("Failed to fetch: HTTP {}", resp.status())));
    }

    let array_buffer = JsFuture::from(resp.array_buffer()?).await?;
    let uint8_array = js_sys::Uint8Array::new(&array_buffer);
    let bytes = uint8_array.to_vec();

    // TODO: Cache in IndexedDB

    Ok(bytes)
}

/// WASM bindings for browser
#[wasm_bindgen]
pub struct Embedder {
    inner: SmartEmbeddingGenerator,
}

#[wasm_bindgen]
impl Embedder {
    /// Create new embedder with automatic backend selection
    ///
    /// Use this factory method instead of `new()` to avoid TypeScript issues
    /// with async constructors.
    ///
    /// # Example
    /// ```javascript
    /// const embedder = await Embedder.create();
    /// ```
    #[wasm_bindgen]
    pub async fn create() -> Result<Embedder, JsValue> {
        let inner = SmartEmbeddingGenerator::new().await;
        Ok(Embedder { inner })
    }

    /// Generate embedding for a single text
    #[wasm_bindgen]
    pub fn embed(&self, text: &str) -> Result<Vec<f32>, JsValue> {
        self.inner.embed(text)
    }

    /// Generate embeddings for multiple texts
    #[wasm_bindgen(js_name = embedBatch)]
    pub fn embed_batch(&self, texts: Vec<String>) -> Result<JsValue, JsValue> {
        let embeddings = self.inner.embed_batch(&texts)?;

        // Convert Vec<Vec<f32>> to JS array
        let js_array = js_sys::Array::new();
        for embedding in embeddings {
            let js_vec = js_sys::Float32Array::from(&embedding[..]);
            js_array.push(&js_vec);
        }

        Ok(js_array.into())
    }

    /// Get embedding dimension
    #[wasm_bindgen]
    pub fn dimension(&self) -> usize {
        self.inner.dimension()
    }

    /// Get model identifier
    #[wasm_bindgen(js_name = modelId)]
    pub fn model_id(&self) -> String {
        match &self.inner {
            SmartEmbeddingGenerator::WebGpu(_) | SmartEmbeddingGenerator::Cpu(_) => {
                BGE_MODEL_ID.to_string()
            }
            SmartEmbeddingGenerator::Hash(_) => "hash-based-v1".to_string(),
        }
    }
}
